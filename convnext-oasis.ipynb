{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004484,"sourceType":"datasetVersion","datasetId":1840540}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anshitavermas/convnext-oasis?scriptVersionId=246756824\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_path = \"/kaggle/input/alzheimer-mri-4-classes-dataset/Alzheimer_MRI_4_classes_dataset\"\n\n# Count images per class\nclass_counts = {}\ntotal_images = 0\n\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    if os.path.isdir(class_dir):\n        num_images = len(os.listdir(class_dir))\n        class_counts[class_name] = num_images\n        total_images += num_images\n\nprint(\"Total Images:\", total_images)\nprint(\"Class Distribution:\", class_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport random\n\ndef show_samples(dataset_path, class_name, num_samples=5):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = os.listdir(class_dir)\n    sample_images = random.sample(images, min(num_samples, len(images)))\n    \n    plt.figure(figsize=(10, 5))\n    for i, img_name in enumerate(sample_images):\n        img_path = os.path.join(class_dir, img_name)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(1, num_samples, i+1)\n        plt.imshow(img)\n        plt.title(class_name)\n        plt.axis(\"off\")\n    \n    plt.show()\n\n# Show samples from each class\nfor class_name in class_counts.keys():\n    show_samples(dataset_path, class_name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install timm albumentations torch torchvision --upgrade albumentations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom albumentations import Compose, Normalize, HorizontalFlip, RandomBrightnessContrast, Resize\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset_path = \"/kaggle/input/alzheimer-mri-4-classes-dataset/Alzheimer_MRI_4_classes_dataset\"\n# device = torch.device(\"cuda\")\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Path to dataset\ndataset_path = \"/kaggle/input/alzheimer-mri-4-classes-dataset/Alzheimer_MRI_4_classes_dataset\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image transformations\ntrain_transform = Compose([\n    Resize(224, 224),  # Resize images to 224x224 (ConvNeXt requirement)\n    HorizontalFlip(p=0.5),  # Random horizontal flip\n    RandomBrightnessContrast(p=0.2),  # Adjust brightness & contrast\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet Normalization\n    ToTensorV2(),  # Convert to PyTorch Tensor\n])\n\nval_transform = Compose([\n    Resize(224, 224),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom albumentations.core.composition import OneOf\n\nclass AlbumentationsDataset(torch.utils.data.Dataset):\n    def __init__(self, folder_path, transform):\n        self.dataset = ImageFolder(folder_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        img, label = self.dataset[idx]\n        img = np.array(img)  # Convert PIL Image to NumPy\n        img = self.transform(image=img)[\"image\"]  # Apply albumentations\n        return img, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset using ImageFolder\nfull_dataset = AlbumentationsDataset(dataset_path, transform=train_transform)\n\n# Split dataset\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\n# Apply validation transforms separately\nval_dataset.dataset.transform = val_transform\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define batch size\nbatch_size = 8\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained ConvNeXt model\nmodel = timm.create_model('convnext_base', pretrained=True, num_classes=4)  # Change classifier for 4 classes\nmodel.to(device)  # Move model to GPU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modify classifier head\nin_features = model.head.fc.in_features\nmodel.head.fc = nn.Linear(in_features, 4)  # 4 output classes\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer):\n    model.train()\n    total_loss, correct = 0, 0\n    total_samples = 0\n\n    for images, labels in tqdm(dataloader, desc=\"Training\"):\n        images, labels = images.to(device), labels.to(device)  # Move to GPU\n\n        optimizer.zero_grad()  # Reset gradients\n        outputs = model(images)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n\n        total_loss += loss.item()\n        correct += (outputs.argmax(1) == labels).sum().item()\n        total_samples += labels.size(0)\n\n    return total_loss / len(dataloader), correct / total_samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss, correct = 0, 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            correct += (outputs.argmax(1) == labels).sum().item()\n            total_samples += labels.size(0)\n\n    return total_loss / len(dataloader), correct / total_samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10\nbest_acc = 0\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    # Save best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), \"best_convnext_model.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_convnext_model.pth\"))\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ndef evaluate_metrics(model, dataloader):\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)  # Get predicted class\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Compute Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # Handle multi-class cases properly\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    sensitivity = recall_score(all_labels, all_preds, average=\"macro\")  # Sensitivity = Recall\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0  # Avoid division by zero\n\n    return accuracy, precision, sensitivity, specificity, cm\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy, precision, sensitivity, specificity, cm = evaluate_metrics(model, val_loader)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Sensitivity (Recall): {sensitivity:.4f}\")\nprint(f\"Specificity: {specificity:.4f}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import datasets\n\n# Load dataset\ndataset = datasets.ImageFolder(root=dataset_path)\n\n# Count images per class\nfrom collections import Counter\nclass_counts = Counter([label for _, label in dataset.samples])\n\n# Get class names\nclass_names = dataset.classes  \n\n# Print results\nfor class_idx, count in class_counts.items():\n    print(f\"Class '{class_names[class_idx]}' has {count} images.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}